{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c66ab1c",
   "metadata": {},
   "source": [
    "## Extended MFRM case study\n",
    "\n",
    "This notebook contains the code to run both a global MFRM analysis (Linacre, 1994) and an extended (matrix) MFRM analysis (Elliott and Buttery, 2022) of a real-world case study, as presented in Elliott and Buttery (2025), in order to highlight the differences in inferences obtained from a global MFRM analysis and an extended MFRM analysis, and also the differences in inferences obtained by selecting a different anchoring frame of raters. The data is from a test of creativity involvin writing rater-scored metaphors about boredom and disgust, originally published in Sylivia & Beaty (2012) and later analysed using the standard (global) MFRM by Primi, Silvia, Jauk and Benedek (2019). The data set for the analyses is available for download at:\n",
    "\n",
    "[http://www.labape.com.br/metaphor/df.xlsx]({http://www.labape.com.br/metaphor/df.xlsx)\n",
    "\n",
    "**References**\n",
    "\n",
    "Elliott, M., & Buttery, P. J. (2022). Extended rater representations in the many-facet Rasch model. *Journal of Applied Measurement*, *22*(1), 133–160.\n",
    "\n",
    "Elliott, M., & Buttery, P. J. (2025). *Addressing non-uniform rater effects with extended many-facet Rasch models: A case study*. Paper to be presented at the Nordic Educational Research Association Conference 2025, Helsinki, Finland, March 5-7.\n",
    "\n",
    "Linacre, J. M. (1994). *Many-Facet Rasch Measurement*. MESA Press.\n",
    "\n",
    "Primi, R., Silvia, P. J., Jauk, E., & Benedek, M. (2019). Applying Many-Facet Rasch Modeling in the Assessment of Creativity. *Psychology of Aesthetics, Creativity, and the Arts*, *13*(2), 176–186.\n",
    "\n",
    "Silvia, P. J., & Beaty, R. E. (2012). Making creative metaphors: The importance of fluid intelligence for\n",
    "creative thought. *Intelligence*, *40*(4), 343–351."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfeeb70-be6e-418a-a836-2df35d4e320c",
   "metadata": {},
   "source": [
    "Import the packages and set the working directory (here called `my_working_directory`) - you need to save the response file here before starting and will also save output files here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1dafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import RaschPy as rp\n",
    "\n",
    "# my_working_directory\n",
    "os.chdir('C:/Users/elliom/Downloads/Chapter_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f3c6b",
   "metadata": {},
   "source": [
    "Load the data and check the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('df.xlsx', header=0)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c784f6-508b-4e37-bf98-72ac3e378ac7",
   "metadata": {},
   "source": [
    "Rescore the data to set the minimum score to 0 (currently 1), reformat the dataframe to the correct format for a *RaschPy* MFRM analysis, and check the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4233bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('subject', inplace=True)\n",
    "data -= 1\n",
    "\n",
    "df_1 = data[['met1_rater1', 'met2_rater1']]\n",
    "df_1.columns = ['Boredom', 'Disgust']\n",
    "\n",
    "df_2 = data[['met1_rater2', 'met2_rater2']]\n",
    "df_2.columns = ['Boredom', 'Disgust']\n",
    "\n",
    "df_3 = data[['met1_rater3', 'met2_rater3']]\n",
    "df_3.columns = ['Boredom', 'Disgust']\n",
    "\n",
    "data_dict = {'Rater_1': df_1, 'Rater_2': df_2, 'Rater_3': df_3}\n",
    "data = pd.concat(data_dict.values(), keys=data_dict.keys())\n",
    "data.index.set_names(['Rater', 'Person'], inplace=True)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70569b90-3db3-41ef-9441-fa12893796d3",
   "metadata": {},
   "source": [
    "Create a *RaschPy* MFRM object from the scores and generate unanchored parameter estimates under the global MFRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm = rp.MFRM(data)\n",
    "mfrm.calibrate_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd213c7-2067-41d6-9e36-d95c4b57e6d9",
   "metadata": {},
   "source": [
    "View the item difficulty estimates, threshold estimates and rater severity estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a96931",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.severities_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8d0c2-f470-4a28-97ac-9a1ceeb60cef",
   "metadata": {},
   "source": [
    "Create item, threshold and rater stats tables, save to file and view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab06772",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.item_stats_df_global()\n",
    "mfrm.item_stats_global.to_csv('item_stats_global_unanchored.csv')\n",
    "mfrm.item_stats_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a622f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.threshold_stats_df_global()\n",
    "mfrm.threshold_stats_global.to_csv('threshold_stats_global_unanchored.csv')\n",
    "mfrm.threshold_stats_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_global()\n",
    "mfrm.rater_stats_global.to_csv('rater_stats_global_unanchored.csv')\n",
    "mfrm.rater_stats_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16794152-a55d-4cc3-97da-e53c3c23543d",
   "metadata": {},
   "source": [
    "Generate plots of item characteristic curve (item response function) and category response curves for *Boredom* and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beac4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.icc_global('Boredom', xmin=-2, xmax=2, title=None,\n",
    "                filename='icc_boredom_rater1_global_unanchored', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.crcs_global('Boredom', xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_rater1_global_unanchored', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8c470-dcc6-4114-9a43-49fe0daebf62",
   "metadata": {},
   "source": [
    "Generate two anchored rater stats tables: firstly anchored to Raters 1 and 2, then anchored to Rater 3, save to file, and view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_global(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.rater_stats_global.to_csv('rater_stats_global_anchored_rater1_rater2.csv')\n",
    "mfrm.rater_stats_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_global(anchor_raters=['Rater_3'])\n",
    "mfrm.rater_stats_global.to_csv('rater_stats_global_anchored_rater3.csv')\n",
    "mfrm.rater_stats_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0cebb-0601-4fdb-ab8f-463305c8d79a",
   "metadata": {},
   "source": [
    "View the bootstrapped standard error estimates for the category widths, both unanchored and anchored. (For the global representation, this will be the same apart from the natural stochastic variation resulting from the bootstrap procedure since the threshold structure is unchanged by the anchoring process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.cat_width_se_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4003221",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.anchor_cat_width_se_global "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b5407-e2e7-49d1-8c16-e26366457f28",
   "metadata": {},
   "source": [
    "Generate unanchored matrix MFRM parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.calibrate_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763555d3-7981-4c5d-8384-d649e0ebd8dc",
   "metadata": {},
   "source": [
    "Generate item, threshold and rater stats dataframes, save to file and view. The rater stats dataframe will, by default (as here), produce the marginal mean severity vectors by item and threshold rather than the full matrix of severities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.item_stats_df_matrix()\n",
    "mfrm.item_stats_matrix.to_csv('item_stats_matrix_unanchored.csv')\n",
    "mfrm.item_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ffe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.threshold_stats_df_matrix()\n",
    "mfrm.threshold_stats_matrix.to_csv('threshold_stats_matrix_unanchored.csv')\n",
    "mfrm.threshold_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a689c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_matrix()\n",
    "mfrm.rater_stats_matrix.to_csv('rater_stats_matrix_unanchored_marginal.csv')\n",
    "mfrm.rater_stats_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14054333-d8a0-4a9d-b408-0a7821119de8",
   "metadata": {},
   "source": [
    "Generate a rater stats dataframe with the full matrix of unanchored severities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34aa72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_matrix(marginal=False)\n",
    "mfrm.rater_stats_matrix.to_csv('rater_stats_matrix_unanchored_full.csv')\n",
    "mfrm.rater_stats_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d738b-9588-4569-9a75-8697c0baabcb",
   "metadata": {},
   "source": [
    "Generate unanchored item characteristic curve and category response curves for *Boredom*, rated by Rater 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.icc_matrix('Boredom', rater='Rater_1', xmin=-2, xmax=2, title=None,\n",
    "                filename='icc_boredom_rater1_matrix_unanchored', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6cc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.crcs_matrix('Boredom', rater='Rater_1', xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_rater1_matrix_unanchored', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40472df4-0353-424f-9161-2aa47b81b73b",
   "metadata": {},
   "source": [
    "Run an anchored matrix calibration with Raters 1 and 2 as the refrence frame; generate item, threshold and rater (with marginal severities) stats dataframes and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199fe11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.calibrate_matrix_anchor(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.item_stats_df_matrix(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.item_stats_matrix.to_csv('item_stats_matrix_anchored_rater1_rater_2.csv')\n",
    "mfrm.item_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b4bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.threshold_stats_df_matrix(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.threshold_stats_matrix.to_csv('threshold_stats_matrix_anchored_rater1_rater_2.csv')\n",
    "mfrm.threshold_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a9fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_matrix(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.rater_stats_matrix.to_csv('rater_stats_matrix_anchored_rater1_rater2_marginal.csv')\n",
    "mfrm.rater_stats_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbec944-152a-4d3c-bf98-3c6993ee30b3",
   "metadata": {},
   "source": [
    "Generate 'neutral rater' category response curves under the matrix representation: first unanchored,than anchored (to Raters 1 and 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.crcs_matrix('Boredom', anchor=False, xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_matrix_unanchored', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.crcs_matrix('Boredom', anchor=True, xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_matrix_anchored', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d072c88-f1e9-4a16-ad03-0127653c602f",
   "metadata": {},
   "source": [
    "Generate category count dataframes, save to file and view (two dataframes generated: overall and by rater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.category_counts_df()\n",
    "mfrm.category_counts.to_csv('category_counts.csv')\n",
    "mfrm.category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac570ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.category_counts_raters.to_csv('category_counts_raters.csv')\n",
    "mfrm.category_counts_raters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fc5ab-8e37-4524-9d5c-c5b412270e37",
   "metadata": {},
   "source": [
    "Generate matrx MFRM parameter estimates anchored to Rater 3; produce item, threshold and rater stats dataframes, save to file and view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea83ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.calibrate_matrix_anchor(anchor_raters=['Rater_3'])\n",
    "mfrm.item_stats_df_matrix(anchor_raters=['Rater_3'])\n",
    "mfrm.item_stats_matrix.to_csv('item_stats_matrix_anchored_rater3.csv')\n",
    "mfrm.item_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.threshold_stats_df_matrix(anchor_raters=['Rater_3'])\n",
    "mfrm.threshold_stats_matrix.to_csv('threshold_stats_matrix_anchored_rater3.csv')\n",
    "mfrm.threshold_stats_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mfrm.rater_stats_df_matrix(anchor_raters=['Rater_3'])\n",
    "mfrm.rater_stats_matrix.to_csv('rater_stats_matrix_anchored_rater3_marginal.csv')\n",
    "mfrm.rater_stats_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750d95d-17cb-4cea-98ed-9b3c27e8edd5",
   "metadata": {},
   "source": [
    "Plot global person estimates versus matrix person estimates, across all raters for non-extreme scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scores = np.arange(17) + 1\n",
    "\n",
    "global_data = [mfrm.score_abil_global(score, raters='all', anchor=True)\n",
    "               for score in scores]\n",
    "matrix_data = [mfrm.score_abil_matrix(score, raters='all', anchor=True)\n",
    "               for score in scores]\n",
    "ax.scatter(global_data, matrix_data, s=30, color='black')\n",
    "\n",
    "plt.plot([-2, 4], [-2, 4], color='darkred', linestyle='dashed')\n",
    "\n",
    "ax.set_aspect('equal', 'box')\n",
    "\n",
    "plt.xticks(np.arange(-5, 5, step=1))\n",
    "plt.yticks(np.arange(-5, 5, step=1))\n",
    "\n",
    "plt.xlim(-2, 4)\n",
    "plt.ylim(-2, 4)\n",
    "\n",
    "plt.xlabel('Global',font='Times', fontsize=15)\n",
    "plt.ylabel('Matrix',font='Times', fontsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('abils_global_v_matrix_all_raters.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9565d-3c29-4a55-a073-1cc2845bc64f",
   "metadata": {},
   "source": [
    "Plot global person estimates versus matrix person estimates, by individual rater for non-extreme scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scores = np.arange(5) + 1\n",
    "\n",
    "def get_data(rater):\n",
    "    \n",
    "    global_data = [mfrm.score_abil_global(score, anchor=True, raters=[rater])\n",
    "                   for score in scores]  \n",
    "    matrix_data = [mfrm.score_abil_matrix(score, anchor=True, raters=[rater])\n",
    "                   for score in scores]\n",
    "    \n",
    "    return global_data, matrix_data\n",
    "\n",
    "global_data_1, matrix_data_1 = get_data('Rater_1')\n",
    "ax.scatter(global_data_1, matrix_data_1, marker='^', s=50, color='darkgrey')\n",
    "\n",
    "global_data_2, matrix_data_2 = get_data('Rater_2')\n",
    "ax.scatter(global_data_2, matrix_data_2, marker='x', s=50, color='black')\n",
    "\n",
    "global_data_3, matrix_data_3 = get_data('Rater_3')\n",
    "ax.scatter(global_data_3, matrix_data_3, marker='+', s=70, color='black')\n",
    "\n",
    "plt.plot([-2, 4], [-2, 4], color='darkred', linestyle='dashed')\n",
    "\n",
    "ax.set_aspect('equal', 'box')\n",
    "\n",
    "plt.xticks(np.arange(-5, 5, step=1))\n",
    "plt.yticks(np.arange(-5, 5, step=1))\n",
    "\n",
    "plt.xlim(-2, 4)\n",
    "plt.ylim(-2, 4)\n",
    "\n",
    "plt.xlabel('Global',font='Times', fontsize=15)\n",
    "plt.ylabel('Matrix',font='Times', fontsize=15)\n",
    "\n",
    "ax.legend(['Rater 1', 'Rater 2', 'Rater 3'])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('abils_global_v_matrix_by_rater.png', dpi=600)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5b317-abbb-46ab-950c-115e00067433",
   "metadata": {},
   "source": [
    "Define function to calculate root mean square (RMS) difference of two arrays of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms (a, b):\n",
    "    \n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    \n",
    "    sq_errors = ((a - b) ** 2).mean()\n",
    "    \n",
    "    return round(np.sqrt(sq_errors), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebde097-fb5d-4384-b1a7-0bbe08b4f7dc",
   "metadata": {},
   "source": [
    "Calculate RMS difference of global versus matrix person estimates across all raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = [mfrm.score_abil_global(score, raters='all', anchor=True)\n",
    "               for score in scores]\n",
    "matrix_data = [mfrm.score_abil_matrix(score, raters='all', anchor=True)\n",
    "               for score in scores]\n",
    "\n",
    "rms(global_data, matrix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273feb39-244f-4b9b-9eff-3254d7176128",
   "metadata": {},
   "source": [
    "Calculate RMS difference of global versus matrix person estimates by individual rater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.arange(5) + 1\n",
    "\n",
    "def get_data(rater):\n",
    "    \n",
    "    global_data = [mfrm.score_abil_global(score, anchor=True, raters=[rater])\n",
    "                   for score in scores]  \n",
    "    matrix_data = [mfrm.score_abil_matrix(score, anchor=True, raters=[rater])\n",
    "                   for score in scores]\n",
    "    \n",
    "    return global_data, matrix_data\n",
    "\n",
    "global_data_1, matrix_data_1 = get_data('Rater_1')\n",
    "print(f'Rater_1: {rms(global_data_1, matrix_data_1)}')\n",
    "\n",
    "global_data_2, matrix_data_2 = get_data('Rater_2')\n",
    "print(f'Rater_2: {rms(global_data_2, matrix_data_2)}')\n",
    "\n",
    "global_data_3, matrix_data_3 = get_data('Rater_3')\n",
    "print(f'Rater_3: {rms(global_data_3, matrix_data_3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ebeca-5951-4e59-9535-16c9fcc5a34b",
   "metadata": {},
   "source": [
    "Plot category response curves for *Boredom* under the matrix MFRM, first anchored to Raters 1 and 2, then anchored to Rater 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.calibrate_matrix_anchor(anchor_raters=['Rater_1', 'Rater_2'])\n",
    "mfrm.crcs_matrix('Boredom', anchor=True, xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_matrix_anchored_rater1_rater2', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.calibrate_matrix_anchor(anchor_raters=['Rater_3'])\n",
    "mfrm.crcs_matrix('Boredom', anchor=True, xmin=-2, xmax=2, title=None,\n",
    "                 filename='crcs_boredom_matrix_anchored_rater3', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459fedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
