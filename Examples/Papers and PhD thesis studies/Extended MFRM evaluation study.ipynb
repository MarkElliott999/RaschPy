{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8f8868-9d8d-49fc-96f8-ab37c47a27e7",
   "metadata": {},
   "source": [
    "## Extended MFRM evaluation study code\n",
    "\n",
    "This Jupyter notebook contains the code needed to run the simulations included in Elliott & Buttery (2022) using `RaschPy` to produce CPAT estimatesunder different MFRM models for comparison. The code produces simulated response data with a set of (by default) 4 anchor raters with no effects and 8 non-anchor raters with a mixture of halo effect and central tendency. Each rater rats a set of `batch_size` (user-definable) individually, and a further set of `batch_size` responses are rated by all raters. The code then produces estimates under global, vector-by-item, vector-by-threshold and matrix MFRMs and produces graphical and statistical comparisons. Experiment parameters are user-definable; full details may be found in Elliott & Buttery (2022).\n",
    "\n",
    "**References**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Elliott, M., & Buttery, P. J. (2022a) Non-iterative Conditional Pairwise Estimation for the Rating Scale Model, *Educational and Psychological Measurement*, *82*(5), 989-1019.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Elliott, M. and Buttery, P. J. (2022b) Extended Rater Representations in the Many-Facet Rasch Model, *Journal of Applied Measurement*, *22*(1), 133-160.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Linacre, J. M. (1994). *Many-Facet Rasch Measurement*. MESA Press.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Luecht, R., & Ackerman, T. A. (2018). A technical note on IRT simulation studies: dealing with truth, estimates, observed data, and residuals. *Educational Measurement: Issues and Practice*, *37*(3), 65–76.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory. *Psychometrika*, *54*(3), 427–450.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce19d0d-1597-42ba-969f-1493bec4d7c0",
   "metadata": {},
   "source": [
    "Import packages and set working directory (change this as appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ddbb2-2b9c-4af3-9408-9999c3d88f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RaschPy as rp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "\n",
    "os.chdir('my_working_directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb01ae2-86a0-4aa6-8fe1-f0e6ae8eea75",
   "metadata": {},
   "source": [
    "Set the high-level parameters for the experiment. These may be adjusted to create different conditions to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0715f66-7d4d-463e-921b-1b5ab6e3a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_items = 8\n",
    "max_score = 5\n",
    "no_of_raters = 12\n",
    "no_of_anchors = 4\n",
    "batch_size = 50\n",
    "sample_size = (no_of_raters + 1) * batch_size\n",
    "add_smoo = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071da11-39bf-49c3-9057-3d0b67bb0d41",
   "metadata": {},
   "source": [
    "Generate item difficulties, thresholds and person abilities. These, together with the rater severities (below) may be adjusted to test different scenarios such as different kinds of rater effects, or the relative effect of the same rater effect on differnt item/threshold structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e3970-dd05-4c4a-81cc-e2b40d8a2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_diffs = np.array([-0.5, -0.5, 0.5, 0.5, -0.5, -0.5, 0.5, 0.5])\n",
    "fixed_thresholds = np.array([0, -2, -1, 0, 1, 2])\n",
    "\n",
    "fixed_abilities = np.random.normal(0, 1.5, sample_size)\n",
    "\n",
    "fixed_ability_series = pd.Series({f'Person {i + 1}': abil\n",
    "                                 for i, abil in enumerate(fixed_abilities)})\n",
    "fixed_ability_series.to_csv(f'person_abilities_{batch_size}.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be71c6-0f42-4f48-9636-d2ff0980671d",
   "metadata": {},
   "source": [
    "Generate rater severities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf3d26-ec67-4fc5-92ce-6ed4154c2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "sevs_items = [0, 0, -1, -1, 0, 0, -1, -1]\n",
    "sevs_items = np.array(sevs_items).astype(np.float64)\n",
    "\n",
    "sevs_thresholds = [0, -2, -1, 0, 1, 2]\n",
    "sevs_thresholds = np.array(sevs_thresholds).astype(np.float64)\n",
    "\n",
    "sevs_anchors = {f'Rater_{rater + 1}':\n",
    "                {f'Item_{item + 1}':\n",
    "                 np.array([0 for thresh in range(max_score + 1)])\n",
    "                 for item in range(no_of_items)}\n",
    "                for rater in range(no_of_anchors)}\n",
    "\n",
    "sevs_non_anchors = {f'Rater_{rater + no_of_anchors + 1}':\n",
    "                    {f'Item_{item + 1}':\n",
    "                     np.array([item_sev + thresh_sev\n",
    "                               if thresh > 0 else 0\n",
    "                               for thresh, thresh_sev in enumerate(sevs_thresholds)])\n",
    "                     for item, item_sev in enumerate(sevs_items)}\n",
    "                    for rater in range(no_of_raters - no_of_anchors)}\n",
    "\n",
    "severities = {**sevs_anchors, **sevs_non_anchors}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c2199-5133-41c5-bf06-fdc8c4c50496",
   "metadata": {},
   "source": [
    "Generate simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e5c8c-7ff6-41ee-b552-22a5f978e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = rp.MFRM_Sim_Matrix(no_of_items=no_of_items, no_of_persons=sample_size, no_of_raters=no_of_raters, \n",
    "                         max_score=max_score, max_disorder=0, offset=0, missing=0,\n",
    "                         manual_abilities=fixed_abilities,\n",
    "                         manual_diffs=fixed_diffs,\n",
    "                         manual_thresholds=fixed_thresholds,\n",
    "                         manual_severities=severities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14caad-43a9-4061-a868-89965b05ec9b",
   "metadata": {},
   "source": [
    "Remove responses from simulation (where all raters rate all persons) to create data design with one set of scripts (n = `batch_size`) rated by all raters and a further n scripts rated by one rater for each rater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe4e5e-ec8e-4d07-9d4e-7694a319b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rater in range(no_of_raters):\n",
    "    for i, person in enumerate(sim.persons):\n",
    "        if (i not in range(batch_size)) & (i not in range((rater + 1) * batch_size,\n",
    "                                                          (rater + 2) * batch_size)):\n",
    "            sim.scores.loc[(f'Rater_{rater + 1}', person), :] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dac7b-22b4-4ba7-ad0c-744011021b2c",
   "metadata": {},
   "source": [
    "Create MFRM object and calibrate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce48fc-4ba9-48b2-9a70-dc5e0e68bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm = rp.MFRM(sim.scores)\n",
    "\n",
    "mfrm.calibrate_global(constant=add_smoo)\n",
    "mfrm.calibrate_items(constant=add_smoo)\n",
    "mfrm.calibrate_thresholds(constant=add_smoo)\n",
    "mfrm.calibrate_matrix(constant=add_smoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff573ac-19ec-4695-8da6-b83016e0c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfrm.calibrate_global_anchor(anchor_raters=['Rater_1', 'Rater_2', 'Rater_3', 'Rater_4'])\n",
    "mfrm.calibrate_items_anchor(anchor_raters=['Rater_1', 'Rater_2', 'Rater_3', 'Rater_4'])\n",
    "mfrm.calibrate_thresholds_anchor(anchor_raters=['Rater_1', 'Rater_2', 'Rater_3', 'Rater_4'])\n",
    "mfrm.calibrate_matrix_anchor(anchor_raters=['Rater_1', 'Rater_2', 'Rater_3', 'Rater_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99228d0-7353-4935-8ad3-58dee718153e",
   "metadata": {},
   "source": [
    "Create a `pandas` series, `base_abils`, which only contains estimates for persons rated by a single rater (we are not interested in the fully crossed set of scripts for or results). Then generate anchored estimates under each model and similarly remove the fully crossed persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9f066-a306-484a-8eca-36f14902cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_abils = sim.abilities.copy()[batch_size:]\n",
    "\n",
    "mfrm.person_abils_global(anchor=True)\n",
    "mfrm.person_abils_items(anchor=True)\n",
    "mfrm.person_abils_thresholds(anchor=True)\n",
    "mfrm.person_abils_matrix(anchor=True)\n",
    "\n",
    "estimates_global = mfrm.anchor_abils_global.loc[base_abils.index]\n",
    "estimates_items = mfrm.anchor_abils_items.loc[base_abils.index]\n",
    "estimates_thresholds = mfrm.anchor_abils_thresholds.loc[base_abils.index]\n",
    "estimates_matrix = mfrm.anchor_abils_matrix.loc[base_abils.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330ec87-1b2d-4d5b-ab29-8524754c2d32",
   "metadata": {},
   "source": [
    "Generate expected score dataframes under the generating parameters and from the estimates for all four models, aexcluding the fully crossed persons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231054f-edd2-421c-ae22-faa7cfc0c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_score_df_generating = {rater: \n",
    "                           pd.DataFrame({person: {item:\n",
    "                                                  mfrm.exp_score_matrix(ability, item, sim.diffs, rater, sim.severities, sim.thresholds)\n",
    "                                                  for item in sim.items}\n",
    "                                         for person, ability in sim.abilities.loc[base_abils.index].items()}).T\n",
    "                           for rater in sim.raters}\n",
    "\n",
    "exp_score_df_generating = pd.concat(exp_score_df_generating.values(), keys=exp_score_df_generating.keys())\n",
    "\n",
    "exp_score_df_global = {rater: \n",
    "                       pd.DataFrame({person: {item:\n",
    "                                              mfrm.exp_score_global(ability, item, mfrm.anchor_diffs_global, rater,\n",
    "                                                                    mfrm.anchor_severities_global, mfrm.anchor_thresholds_global)\n",
    "                                              for item in sim.items}\n",
    "                                     for person, ability in sim.abilities.loc[base_abils.index].items()}).T\n",
    "                       for rater in sim.raters}\n",
    "\n",
    "exp_score_df_global = pd.concat(exp_score_df_global.values(), keys=exp_score_df_global.keys())\n",
    "\n",
    "exp_score_df_items = {rater: \n",
    "                      pd.DataFrame({person: {item:\n",
    "                                             mfrm.exp_score_items(ability, item, mfrm.anchor_diffs_items, rater,\n",
    "                                                                  mfrm.anchor_severities_items, mfrm.anchor_thresholds_items)\n",
    "                                             for item in sim.items}\n",
    "                                    for person, ability in sim.abilities.loc[base_abils.index].items()}).T\n",
    "                      for rater in sim.raters}\n",
    "\n",
    "exp_score_df_items = pd.concat(exp_score_df_items.values(), keys=exp_score_df_items.keys())\n",
    "\n",
    "exp_score_df_thresholds = {rater: \n",
    "                           pd.DataFrame({person: {item:\n",
    "                                                  mfrm.exp_score_thresholds(ability, item, mfrm.anchor_diffs_thresholds,\n",
    "                                                                            rater, mfrm.anchor_severities_thresholds,\n",
    "                                                                            mfrm.anchor_thresholds_thresholds)\n",
    "                                                  for item in sim.items}\n",
    "                                         for person, ability in sim.abilities.loc[base_abils.index].items()}).T\n",
    "                           for rater in sim.raters}\n",
    "\n",
    "exp_score_df_thresholds = pd.concat(exp_score_df_thresholds.values(), keys=exp_score_df_thresholds.keys())\n",
    "\n",
    "exp_score_df_matrix = {rater: \n",
    "                       pd.DataFrame({person: {item:\n",
    "                                              mfrm.exp_score_matrix(ability, item, mfrm.anchor_diffs_matrix, rater,\n",
    "                                                                    mfrm.anchor_severities_matrix, mfrm.anchor_thresholds_matrix)\n",
    "                                              for item in sim.items}\n",
    "                                     for person, ability in sim.abilities.loc[base_abils.index].items()}).T\n",
    "                       for rater in sim.raters}\n",
    "\n",
    "exp_score_df_matrix = pd.concat(exp_score_df_matrix.values(), keys=exp_score_df_matrix.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd7ea4-fa46-4a39-aae2-a472bc7ce22a",
   "metadata": {},
   "source": [
    "Define function to calculate ability estimate from responses under the matrix MFRM, passing whatever set of item/threshold/rater parameters are relevant, plus corresponding function for Warm (1989) bias correction. This function is used to create the pseudo-estimates: ability estimates derived from the generating item, threshold and rater parameters and the response dataframe. Pseudo-estimates effectively remove the stochastic element from comparisons, in the spirit of parameter-estimation residuals (Luecht & Ackerman, 2018). See Elliott & Buttery (2022b) for a full discussion of pseudo-estimates.\n",
    "\n",
    "**NOTE** 22/1/2025: This function still needs to be added to the `RaschPy` codebase as a proper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954f006-dc03-4181-b4b5-402f6395dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abil_matrix(scores,\n",
    "                person,\n",
    "                rater,\n",
    "                diffs,\n",
    "                thresholds,\n",
    "                severities,\n",
    "                warm_corr=False,\n",
    "                tolerance=0.0000001,\n",
    "                max_iter=100,\n",
    "                ext_score_adjustment=0.3):\n",
    "    \n",
    "    ext_score = sim.max_score * sim.no_of_items\n",
    "    \n",
    "    score = scores.xs(rater).sum(axis=1)[person]\n",
    "    \n",
    "    if score == 0:\n",
    "        used_score = ext_score_adjustment\n",
    "\n",
    "    elif score == ext_score:\n",
    "        used_score = score - ext_score_adjustment\n",
    "\n",
    "    else:\n",
    "        used_score = score\n",
    "\n",
    "    estimate = log(used_score) - log(ext_score - used_score)\n",
    "    \n",
    "    change = 1\n",
    "    iterations = 0\n",
    "    \n",
    "    while (abs(change) > tolerance) & (iterations <= max_iter):\n",
    "        \n",
    "        result = sum(mfrm.exp_score_matrix(estimate,\n",
    "                                           item,\n",
    "                                           diffs,\n",
    "                                           rater,\n",
    "                                           severities,\n",
    "                                           thresholds)\n",
    "                     for item in mfrm.items)\n",
    "                \n",
    "        info = sum(mfrm.variance_matrix(estimate,\n",
    "                                        item,\n",
    "                                        diffs,\n",
    "                                        rater,\n",
    "                                        severities,\n",
    "                                        thresholds)\n",
    "                   for item in mfrm.items)\n",
    "\n",
    "        change = max(-1, min(1, (result - used_score) / info))\n",
    "        estimate -= change\n",
    "        iterations += 1\n",
    "\n",
    "    if warm_corr:\n",
    "        estimate += warm_matrix(estimate,\n",
    "                                diffs,\n",
    "                                rater,\n",
    "                                severities,\n",
    "                                thresholds)\n",
    "    \n",
    "    return estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2a5d0-7166-4b10-bac6-4c988fb88ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_matrix(estimate,\n",
    "                diffs,\n",
    "                rater,\n",
    "                severities,\n",
    "                thresholds):\n",
    "\n",
    "    '''\n",
    "    Warm's (1989) bias correction for ML abiity estimates\n",
    "    '''\n",
    "\n",
    "    exp_scores = [mfrm.exp_score_matrix(estimate,\n",
    "                                        item,\n",
    "                                        diffs,\n",
    "                                        rater,\n",
    "                                        severities,\n",
    "                                        thresholds)\n",
    "                  for item in mfrm.items]\n",
    "\n",
    "    variances = [mfrm.variance_matrix(estimate,\n",
    "                                      item,\n",
    "                                      diffs,\n",
    "                                      rater,\n",
    "                                      severities,\n",
    "                                      thresholds)\n",
    "                 for item in mfrm.items]\n",
    "        \n",
    "    part_1 = sum(sum((cat ** 3) * mfrm.cat_prob_matrix(estimate,\n",
    "                                                       item,\n",
    "                                                       diffs,\n",
    "                                                       rater,\n",
    "                                                       severities,\n",
    "                                                       cat,\n",
    "                                                       thresholds)\n",
    "                     for cat in range(mfrm.max_score + 1))\n",
    "                 for item in mfrm.items)\n",
    "\n",
    "    part_2 = 3 * sum((variances[i] + (exp_scores[i] ** 2)) *\n",
    "                     exp_scores[i]\n",
    "                     for i, item in enumerate(mfrm.items))\n",
    "\n",
    "    part_3 = sum(2 * (exp_scores[i] ** 3)\n",
    "                 for i, item in enumerate(mfrm.items))\n",
    "\n",
    "    warm_correction = ((part_1 - part_2 + part_3) /\n",
    "                       (2 * (sum(variances[i]\n",
    "                              for i, item in enumerate(mfrm.items)) ** 2)))\n",
    "\n",
    "    return warm_correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218e347-6f54-4df8-a1a1-7c506969c421",
   "metadata": {},
   "source": [
    "Generate pseudo-estimates (not including persons rated by all raters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c113bb-24b2-49f2-8180-d9114ebea1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import trunc\n",
    "from math import log\n",
    "\n",
    "pseudo_estimates = {}\n",
    "\n",
    "for i, person in enumerate(base_abils.index):\n",
    "    rater_no = trunc(i / batch_size) + 1\n",
    "    rater = f'Rater_{rater_no}'\n",
    "    \n",
    "    pseudo_estimates[person] = abil_matrix(sim.scores, person, rater, sim.diffs, sim.thresholds,\n",
    "                                           sim.severities, warm_corr=True)\n",
    "    pseudo_estimates = pd.Series(pseudo_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63eec06-5981-4573-9637-9085076e4d33",
   "metadata": {},
   "source": [
    "Plot pseudo-estimates against the four sets of estimates under the different models (global, vector by item, vector by threshold, matrix) and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9579fbd-7978-44d8-8db0-a6d39507eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "plt.plot([-6, 6], [-6, 6], color='k', linestyle='--', linewidth=2, zorder=0)\n",
    "ax.scatter(pseudo_estimates[:batch_size * no_of_anchors],\n",
    "           estimates_global[:batch_size * no_of_anchors],\n",
    "           label=\"Anchor raters\",\n",
    "           marker='^',\n",
    "           color='darkblue', s=40)\n",
    "ax.scatter(pseudo_estimates[batch_size * no_of_anchors:],\n",
    "           estimates_global[batch_size * no_of_anchors:],\n",
    "           label=\"Non-anchor raters\",\n",
    "           marker='v',\n",
    "           color='darkred', s=40)\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "plt.title('Global rater representation', {'fontsize': 40})\n",
    "plt.xlabel('Pseudo-estimates', {'fontsize': 30})\n",
    "plt.ylabel('Estimates', {'fontsize': 30})\n",
    "\n",
    "ax.legend(loc='lower right', fontsize=24, markerscale=3, bbox_to_anchor=(0.98, 0.09))\n",
    "\n",
    "plt.savefig(f'scatter_abils_residual_matrix_{batch_size}.png',\n",
    "            dpi = 200,\n",
    "            bbox_inches = 'tight',\n",
    "            pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77964c7a-8c36-4dda-b3c5-e335788cffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "plt.plot([-6, 6], [-6, 6], color='k', linestyle='--', linewidth=2, zorder=0)\n",
    "ax.scatter(pseudo_estimates[:batch_size * no_of_anchors],\n",
    "           estimates_items[:batch_size * no_of_anchors],\n",
    "           label=\"Anchor raters\",\n",
    "           marker='^',\n",
    "           color='darkblue', s=40)\n",
    "ax.scatter(pseudo_estimates[batch_size * no_of_anchors:],\n",
    "           estimates_items[batch_size * no_of_anchors:],\n",
    "           label=\"Non-anchor raters\",\n",
    "           marker='v',\n",
    "           color='darkred', s=40)\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "plt.title('Vector-by-item rater representation', {'fontsize': 40})\n",
    "plt.xlabel('Pseudo-estimates', {'fontsize': 30})\n",
    "plt.ylabel('Estimates', {'fontsize': 30})\n",
    "\n",
    "ax.legend(loc='lower right', fontsize=24, markerscale=3, bbox_to_anchor=(0.98, 0.09))\n",
    "\n",
    "plt.savefig(f'scatter_abils_residual_matrix_{batch_size}.png',\n",
    "            dpi = 200,\n",
    "            bbox_inches = 'tight',\n",
    "            pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1750241-a1ad-495a-85fa-b6db055868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "plt.plot([-6, 6], [-6, 6], color='k', linestyle='--', linewidth=2, zorder=0)\n",
    "ax.scatter(pseudo_estimates[:batch_size * no_of_anchors],\n",
    "           estimates_thresholds[:batch_size * no_of_anchors],\n",
    "           label=\"Anchor raters\",\n",
    "           marker='^',\n",
    "           color='darkblue', s=40)\n",
    "ax.scatter(pseudo_estimates[batch_size * no_of_anchors:],\n",
    "           estimates_thresholds[batch_size * no_of_anchors:],\n",
    "           label=\"Non-anchor raters\",\n",
    "           marker='v',\n",
    "           color='darkred', s=40)\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "plt.title('Vector-by-threshold rater representation', {'fontsize': 40})\n",
    "plt.xlabel('Pseudo-estimates', {'fontsize': 30})\n",
    "plt.ylabel('Estimates', {'fontsize': 30})\n",
    "\n",
    "ax.legend(loc='lower right', fontsize=24, markerscale=3, bbox_to_anchor=(0.98, 0.09))\n",
    "\n",
    "plt.savefig(f'scatter_abils_residual_matrix_{batch_size}.png',\n",
    "            dpi = 200,\n",
    "            bbox_inches = 'tight',\n",
    "            pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928ce3b-cf31-4608-937c-50099fdfb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "plt.plot([-6, 6], [-6, 6], color='k', linestyle='--', linewidth=2, zorder=0)\n",
    "ax.scatter(pseudo_estimates[:batch_size * no_of_anchors],\n",
    "           estimates_matrix[:batch_size * no_of_anchors],\n",
    "           label=\"Anchor raters\",\n",
    "           marker='^',\n",
    "           color='darkblue', s=40)\n",
    "ax.scatter(pseudo_estimates[batch_size * no_of_anchors:],\n",
    "           estimates_matrix[batch_size * no_of_anchors:],\n",
    "           label=\"Non-anchor raters\",\n",
    "           marker='v',\n",
    "           color='darkred', s=40)\n",
    "\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(-6, 6)\n",
    "\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "plt.title('Matrix rater representation', {'fontsize': 40})\n",
    "plt.xlabel('Pseudo-estimates', {'fontsize': 30})\n",
    "plt.ylabel('Estimates', {'fontsize': 30})\n",
    "\n",
    "ax.legend(loc='lower right', fontsize=24, markerscale=3, bbox_to_anchor=(0.98, 0.09))\n",
    "\n",
    "plt.savefig(f'scatter_abils_residual_matrix_{batch_size}.png',\n",
    "            dpi = 200,\n",
    "            bbox_inches = 'tight',\n",
    "            pad_inches = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377d14f-578f-48ec-acfc-b9ee04d52d73",
   "metadata": {},
   "source": [
    "**Statistical output**\n",
    "\n",
    "Define functions for RMSE, SD ratio and RMS parameter estimation residual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846508c6-08f3-4833-bb73-bd7bbec97a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "\n",
    "    mse = ((x - y) ** 2).mean()\n",
    "    \n",
    "    return np.sqrt(mse)\n",
    "\n",
    "def sd_ratio(x, y):\n",
    "\n",
    "    return y.std() / x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5adc40-cf72-4eef-b71d-1ada9550b86e",
   "metadata": {},
   "source": [
    "Calculate RMSEs for parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dec367-fee8-46d3-a60f-ccf0428d73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_items_global = rmse(mfrm.anchor_diffs_global, sim.diffs)\n",
    "rmse_items_items = rmse(mfrm.anchor_diffs_items, sim.diffs)\n",
    "rmse_items_thresholds = rmse(mfrm.anchor_diffs_thresholds, sim.diffs)\n",
    "rmse_items_matrix = rmse(mfrm.anchor_diffs_matrix, sim.diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6ebf1-6c1e-443d-b4fc-9f12f66a6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_thresholds_global = rmse(mfrm.anchor_thresholds_global, sim.thresholds)\n",
    "rmse_thresholds_items = rmse(mfrm.anchor_thresholds_items, sim.thresholds)\n",
    "rmse_thresholds_thresholds = rmse(mfrm.anchor_thresholds_thresholds, sim.thresholds)\n",
    "rmse_thresholds_matrix = rmse(mfrm.anchor_thresholds_matrix, sim.thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd5fb9-7680-4821-8487-abb31c65b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sevs_dict = {rater: pd.DataFrame(sim.severities[rater]).iloc[1:] for rater in sim.raters}\n",
    "sevs_df = pd.concat(sevs_dict.values(), keys=sevs_dict.keys())\n",
    "\n",
    "sevs_dict_items = {rater: sevs_dict[rater].iloc[1:].mean()\n",
    "                   for rater in sim.raters}\n",
    "sevs_df_items = pd.DataFrame(sevs_dict_items)\n",
    "sevs_series_items = sevs_df_items.unstack()\n",
    "rmse_severities_items = rmse(pd.DataFrame(mfrm.anchor_severities_items).unstack(), sevs_series_items)\n",
    "\n",
    "sevs_dict_thresholds = {rater: sevs_dict[rater].T.iloc[:, 1:].mean()\n",
    "                   for rater in sim.raters}\n",
    "sevs_series_thresholds = pd.DataFrame(sevs_dict_thresholds).unstack()\n",
    "rmse_severities_thresholds = rmse(pd.DataFrame(mfrm.anchor_severities_thresholds).iloc[1:].unstack(), sevs_series_thresholds)\n",
    "\n",
    "sevs_dict_matrix = {rater: pd.DataFrame(mfrm.anchor_severities_matrix[rater]).iloc[1:] for rater in sim.raters}\n",
    "sevs_df_matrix = pd.concat(sevs_dict_matrix.values(), keys=sevs_dict_matrix.keys())\n",
    "sevs_series_matrix = sevs_df_matrix.unstack().unstack()\n",
    "rmse_severities_matrix = rmse(sevs_df.unstack().unstack(), sevs_series_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ae197-432c-4076-b997-82dff3b67b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_abilities_global = rmse(estimates_global, base_abils)\n",
    "rmse_abilities_items = rmse(estimates_items, base_abils)\n",
    "rmse_abilities_thresholds = rmse(estimates_thresholds, base_abils)\n",
    "rmse_abilities_matrix = rmse(estimates_matrix, base_abils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ed87f-e9cf-4fc2-86f9-d4e4b6953711",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_ability_residuals_global = rmse(estimates_global, pseudo_estimates)\n",
    "rms_ability_residuals_items = rmse(estimates_items, pseudo_estimates)\n",
    "rms_ability_residuals_thresholds = rmse(estimates_thresholds, pseudo_estimates)\n",
    "rms_ability_residuals_matrix = rmse(estimates_matrix, pseudo_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0122edb5-b47a-4c62-b7c0-5578b4a0167b",
   "metadata": {},
   "source": [
    "Calculate parameter-estimation residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5f90f-a7b5-4f91-a745-7b2ee78cd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = mfrm.dataframe.loc[(slice(None), base_abils.index), :]\n",
    "\n",
    "residuals_generating = scores_df - exp_score_df_generating\n",
    "residuals_global = scores_df - exp_score_df_global\n",
    "residuals_items = scores_df - exp_score_df_items\n",
    "residuals_thresholds = scores_df - exp_score_df_thresholds\n",
    "residuals_matrix = scores_df - exp_score_df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a24fd-7367-4529-8036-7b835976ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_est_residuals_global = residuals_global - residuals_generating\n",
    "param_est_residuals_items = residuals_items - residuals_generating\n",
    "param_est_residuals_thresholds = residuals_thresholds - residuals_generating\n",
    "param_est_residuals_matrix = residuals_matrix - residuals_generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111424f-c030-4ebc-bafc-03e9209b7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_est_residuals_global = param_est_residuals_global.unstack().unstack().dropna()\n",
    "param_est_residuals_items = param_est_residuals_items.unstack().unstack().dropna()\n",
    "param_est_residuals_thresholds = param_est_residuals_thresholds.unstack().unstack().dropna()\n",
    "param_est_residuals_matrix = param_est_residuals_matrix.unstack().unstack().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fdcee-9f9b-4d2e-8d3b-12bf3d77405a",
   "metadata": {},
   "source": [
    "Calculate RMS of parameter-estimation residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1e889-c391-4261-959b-1478aaceaeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_param_est_residuals_global = np.sqrt((param_est_residuals_global ** 2).mean())\n",
    "rms_param_est_residuals_items = np.sqrt((param_est_residuals_items ** 2).mean())\n",
    "rms_param_est_residuals_thresholds = np.sqrt((param_est_residuals_thresholds ** 2).mean())\n",
    "rms_param_est_residuals_matrix = np.sqrt((param_est_residuals_matrix ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628073cc-d4df-426d-bb41-fff8a5a9f9b4",
   "metadata": {},
   "source": [
    "Generate results table and save to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c4298-2767-4ff5-ad2b-39f3ff2b86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['Global'] = {'RMSE items': rmse_items_global,\n",
    "                        'RMSE thresholds': rmse_thresholds_global,\n",
    "                        'RMSE severities': '',\n",
    "                        'RMSE abilities': rmse_abilities_global,\n",
    "                        'RMS ability residuals': rms_ability_residuals_global,\n",
    "                        'RMS Parameter estimation residual': rms_param_est_residuals_global}\n",
    "results_df['Items'] = {'RMSE items': rmse_items_items,\n",
    "                       'RMSE thresholds': rmse_thresholds_items,\n",
    "                       'RMSE severities': rmse_severities_items,\n",
    "                       'RMSE abilities': rmse_abilities_items,\n",
    "                       'RMS ability residuals': rms_ability_residuals_items,\n",
    "                       'RMS Parameter estimation residual': rms_param_est_residuals_items}\n",
    "results_df['Thresholds'] = {'RMSE items': rmse_items_thresholds,\n",
    "                            'RMSE thresholds': rmse_thresholds_thresholds,\n",
    "                            'RMSE severities': rmse_severities_thresholds,\n",
    "                            'RMSE abilities': rmse_abilities_thresholds,\n",
    "                            'RMS ability residuals': rms_ability_residuals_thresholds,\n",
    "                            'RMS Parameter estimation residual': rms_param_est_residuals_thresholds}\n",
    "results_df['Matrix'] = {'RMSE items': rmse_items_matrix,\n",
    "                        'RMSE thresholds': rmse_thresholds_matrix,\n",
    "                        'RMSE severities': rmse_severities_matrix,\n",
    "                        'RMSE abilities': rmse_abilities_matrix,\n",
    "                        'RMS ability residuals': rms_ability_residuals_matrix,\n",
    "                        'RMS Parameter estimation residual': rms_param_est_residuals_matrix}\n",
    "\n",
    "results_df.to_csv(f'Results_{batch_size}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14a041-bce9-48dc-a297-8650810ed76c",
   "metadata": {},
   "source": [
    "View results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dade2b-abbb-4774-81cf-ab84c242b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(results_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203c1fd-556a-4823-beb4-083a4aa0e151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
